{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxHvtoJzVk90ylCS+iZeuL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/machine-learning-notes/blob/main/llm/transformers/positional_encoding/positional_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Positional Encoding**\n",
        "\n",
        "- To add sequential order in the input embedding.\n",
        "- Unlike RNN, LSTM. Transformer architecture not follow seqeuntial order.\n",
        "- In Natural Languages order is important.\n",
        "- So for that we explicitly inject position in embedding."
      ],
      "metadata": {
        "id": "96fM09VvuikZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M_btju_EtFti"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab: list[str] = [\"i\", \"love\", \"biriyani\"]\n",
        "text: str = \"i love biriyani\"\n",
        "\n",
        "vocab_idx = [vocab.index(each) for each in text.split(\" \")]\n",
        "vocab_idx = torch.tensor(vocab_idx)\n",
        "vocab_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJaLCe-Sx2hC",
        "outputId": "cd36525e-cb72-435e-ba97-b0a2814f8fd6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedd = torch.nn.Embedding(num_embeddings=3, embedding_dim=5)\n",
        "embedded_ip = embedd(vocab_idx)\n",
        "embedded_ip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2TA33gQyhqW",
        "outputId": "fc6b9392-7822-4740-b057-7c32415ced44"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2.0258, -0.5164,  2.2561,  0.5965, -1.0522],\n",
              "        [ 1.2649, -0.3838,  0.0201,  0.3150,  1.7188],\n",
              "        [-0.5363, -1.1554,  2.0050,  1.3424, -0.6690]],\n",
              "       grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PE = sin(pos / (10000 ^ (2i)/d_model))  : for even\n",
        "# PE = cos(pos / (10000 ^ (2i)/d_model))  : for odd\n",
        "#\n",
        "#  pos     : position of the vector in embedding\n",
        "#  i       : position of each value of the pos vector\n",
        "#  d_model : similar to  embedding_dim"
      ],
      "metadata": {
        "id": "KAhIJKFWz18S"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sin_encoder(\n",
        "        pos: int,\n",
        "        d_model: int,\n",
        "        i: int) -> torch.Tensor:\n",
        "    val = pos / torch.pow(torch.tensor(10000), (torch.tensor(2*i)/d_model))\n",
        "\n",
        "    return torch.sin(val)"
      ],
      "metadata": {
        "id": "3iASlGaY9XOA"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_encoder(\n",
        "        pos: int,\n",
        "        d_model: int,\n",
        "        i: int) -> torch.Tensor:\n",
        "    val = pos / torch.pow(torch.tensor(10000), (torch.tensor(2*i)/d_model))\n",
        "\n",
        "    return torch.cos(val)"
      ],
      "metadata": {
        "id": "AG43GFBh_PPD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 5 # d_model is just embedding_dim\n",
        "result = []\n",
        "for pos, vec in enumerate(embedded_ip):\n",
        "    for i, each in enumerate(vec):\n",
        "        if i % 2 == 0:\n",
        "            result.append([sin_encoder(pos, d_model, i)])\n",
        "        else:\n",
        "            result.append([cos_encoder(pos, d_model, i)])\n",
        "\n",
        "positional_encoding = torch.tensor(result).reshape(-1, 5)\n",
        "positional_encoding # it same shape as the embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kuKAsNP-fty",
        "outputId": "a0c582a1-bf7a-41bd-b0ea-265cf2a695fc"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
              "        [8.4147e-01, 9.9968e-01, 6.3096e-04, 1.0000e+00, 3.9811e-07],\n",
              "        [9.0930e-01, 9.9874e-01, 1.2619e-03, 1.0000e+00, 7.9621e-07]])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just add the positional_encoding to the emebddings\n",
        "\n",
        "print(f\"Embedding : \\n{embedded_ip}\")\n",
        "pe_embedding = embedded_ip + positional_encoding\n",
        "print(f\"\\n\\nEmbedding after add Positioinal Encode : \\n{pe_embedding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo--Hxn_Eeiu",
        "outputId": "fe4ed624-6ac2-4bc1-c5d9-852c83d0f5e8"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding : \n",
            "tensor([[ 2.0258, -0.5164,  2.2561,  0.5965, -1.0522],\n",
            "        [ 1.2649, -0.3838,  0.0201,  0.3150,  1.7188],\n",
            "        [-0.5363, -1.1554,  2.0050,  1.3424, -0.6690]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "\n",
            "Embedding after add Positioinal Encode : \n",
            "tensor([[ 2.0258,  0.4836,  2.2561,  1.5965, -1.0522],\n",
            "        [ 2.1063,  0.6159,  0.0208,  1.3150,  1.7188],\n",
            "        [ 0.3730, -0.1567,  2.0062,  2.3424, -0.6690]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**the added value doest have reverse equation to calcualte the position.**\n",
        "\n",
        "**The model learn the positional features during training.**"
      ],
      "metadata": {
        "id": "jOhLW4CtFWg4"
      }
    }
  ]
}