{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xDz4PSnylXQf",
        "2vHEKzznJcM-"
      ],
      "authorship_tag": "ABX9TyPb61dbVg+XN2J8iK0BLDZT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/machine-learning-notes/blob/main/llm/transformers/transformer/transformer_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Implementation Using PyTorch**"
      ],
      "metadata": {
        "id": "gCREwa9mIr3w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qL8xgQmMImUc"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A Transformer consists of a stack of encoders and decoders.\n",
        "- However, modern LLMs like `GPT` `do not` use the full `encoderâ€“decoder architecture`.\n",
        "- `GPT` models are based only on the `decoder block`.\n",
        "- I implemented this according to the [**Attention Is All You Need paper (2017)**](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) *by Google*.\n",
        "- I chose **PyTorch over NumPy** because it provides a deeper understanding of tensors and is widely used in production-level LLM development.\n",
        "- **We'll implement the core components from scratch.**"
      ],
      "metadata": {
        "id": "ah9OGxdCIzZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **setup input data**"
      ],
      "metadata": {
        "id": "xJl5VTwIiaCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# token size      : 100\n",
        "# embeddings size : 10\n",
        "\n",
        "input_token = torch.randn([100, 10])\n",
        "input_token.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_1oJmPAiZxx",
        "outputId": "be2bb138-5f81-4a93-fda3-4ea347745e6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper Functions**"
      ],
      "metadata": {
        "id": "OuZKzXuChPqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding() -> torch.Tensor:\n",
        "    pass"
      ],
      "metadata": {
        "id": "k0EY6VuUhO9p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Self Attention**"
      ],
      "metadata": {
        "id": "8G90rtXAm5Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_score(\n",
        "        q: torch.Tensor,\n",
        "        k: torch.Tensor,\n",
        "        v: torch.Tensor,\n",
        "        d_model: int,\n",
        "        num_of_heads: int,\n",
        "        masking: torch.Tensor | None = None\n",
        "        ) -> torch.Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        Attention (Q, K, V) = Softmax((Q * K^T) / root of d_k) * V\n",
        "        \"\"\"\n",
        "        d_k = torch.tensor(d_model // num_of_heads) # no.of feature per head\n",
        "\n",
        "        val_1 = q @ k.T / torch.sqrt(d_k)\n",
        "        if masking:\n",
        "            val_1 += masking\n",
        "        return torch.softmax(val_1, dim=1) @ v"
      ],
      "metadata": {
        "id": "koAWdHbxm5IF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multi Head Attention**"
      ],
      "metadata": {
        "id": "xDz4PSnylXQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int) -> None:\n",
        "        super().__init__(self)\n",
        "        self.d_model = d_model\n",
        "        self.num_of_heads = num_heads\n",
        "\n",
        "        self.q_w = torch.nn.Linear(d_model, d_model)\n",
        "        self.k_w = torch.nn.Linear(d_model, d_model)\n",
        "        self.v_w = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.d_k = self.d_model // self.num_of_heads\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        self.seq_len = x.shape[0]\n",
        "\n",
        "        Q = self.q_w(x)\n",
        "        K = self.k_w(x)\n",
        "        V = self.v_w(x)\n",
        "\n",
        "        split_q = Q.view(self.seq_len, self.num_heads, self.d_k).transpose(0, 1)\n",
        "        split_k = K.view(self.seq_len, self.num_heads, self.d_k).transpose(0, 1)\n",
        "        split_v = V.view(self.seq_len, self.num_heads, self.d_k).transpose(0, 1)\n",
        "\n",
        "        heads_scores = []\n",
        "        for q, k, v in zip(split_q, split_k, split_v):\n",
        "            score = get_attention_score(\n",
        "                q=q, k=k, v=v, d_model=self.d_model, num_of_heads=self.num_of_heads\n",
        "                )\n",
        "\n",
        "            heads_scores.append(score)\n",
        "\n",
        "        tensor_scores = torch.stack(heads_scores, dim=0)\n",
        "\n",
        "        merged_scores = tensor_scores.transpose(0, 1).reshape(self.seq_len, self.d_model)\n",
        "\n",
        "        return merged_scores"
      ],
      "metadata": {
        "id": "mToQEXCClW_x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Masked Multi Head Attention**"
      ],
      "metadata": {
        "id": "X8eCCjCbN47k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedMultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int) -> None:\n",
        "        super().__init__(self)\n",
        "        self.d_model = d_model\n",
        "        self.num_of_heads = num_heads\n",
        "\n",
        "        self.q_w = torch.nn.Linear(d_model, d_model)\n",
        "        self.k_w = torch.nn.Linear(d_model, d_model)\n",
        "        self.v_w = torch.nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.d_k = self.d_model // self.num_of_heads\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        self.seq_len = x.shape[0]\n",
        "\n",
        "        Q = self.q_w(x)\n",
        "        K = self.k_w(x)\n",
        "        V = self.v_w(x)\n",
        "\n",
        "        split_q = Q.view(self.seq_len, self.num_heads, self.d_k).transpose(0, 1)\n",
        "        split_k = K.view(self.seq_len, self.num_heads, self.d_k).transpose(0, 1)\n",
        "        split_v = V.view(self.seq_len, self.num_heads, self.d_k).transpose(0, 1)\n",
        "\n",
        "        heads_scores = []\n",
        "        for q, k, v in zip(split_q, split_k, split_v):\n",
        "            score = get_attention_score(\n",
        "                q=q, k=k, v=v, d_model=self.d_model, num_of_heads=self.num_of_heads\n",
        "                )\n",
        "\n",
        "            heads_scores.append(score)\n",
        "\n",
        "        tensor_scores = torch.stack(heads_scores, dim=0)\n",
        "\n",
        "        merged_scores = tensor_scores.transpose(0, 1).reshape(self.seq_len, self.d_model)\n",
        "\n",
        "        return merged_scores"
      ],
      "metadata": {
        "id": "Ns_NXGFPN4rX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoder Block**"
      ],
      "metadata": {
        "id": "2vHEKzznJcM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int) -> None:\n",
        "        super().__init__(self)\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=1)\n",
        "        self.norm1 = torch.nn.LayerNorm()\n",
        "        self.norm1 = torch.nn.LayerNorm()\n",
        "        self.ffn = torch.nn.Sequence(\n",
        "            torch.nn.Linear(d_model, d_model * 4),\n",
        "            torch.ReLu(),\n",
        "            torch.nn.Linear(d_model * 4, d_model * 4),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        mha_out = self.mha(x) # multi-head attention block\n",
        "        norm1_result = self.norm1(mha_out + x) # residual conncetion + layer norm\n",
        "        ffn_result = self.ffn(norm1_result) # ffn (2 layer mlp)\n",
        "        norm2_result = self.norm2(ffn_result+x) # residual connection + layer norm\n",
        "\n",
        "        return norm2_result\n"
      ],
      "metadata": {
        "id": "_iv8lcdnJhI8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decoder Block**"
      ],
      "metadata": {
        "id": "jo_DdQLzJeb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__(self)\n",
        "\n",
        "    def forward(self) -> torch.Tensor:\n",
        "        pass"
      ],
      "metadata": {
        "id": "LsbMbCyBIyxS"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}