{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYzqaS8Jmc3JTNrZOtGR4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/machine-learning-notes/blob/main/DEEP_LEARNING/multilayer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MultiLayer Perceptron from Scratch (using NumPy)**"
      ],
      "metadata": {
        "id": "_7AIDw0DyLQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **MLP is a Multi Layer Perceptron Architecture**\n",
        "- **weight initialize : W=(next layer features,current layer features)**"
      ],
      "metadata": {
        "id": "xj6u8sHCzwun"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7vy6haLyFyM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(2342)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# util function\n",
        "\n",
        "def relu(z: np.ndarray | float | int) -> np.ndarray | float | int:\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def derivative_sigmoid(\n",
        "        z: np.ndarray | float | int) -> np.ndarray | float | int:\n",
        "        return z * (1-z)\n",
        "\n",
        "def derivative_relu(\n",
        "        z: np.ndarray | float | int) -> np.ndarray | float | int:\n",
        "        return (z > 0).astype(float)\n",
        "\n",
        "def sigmoid(z: np.ndarray):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "def derivative_bce(z: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    return (z - y) / (z * (1 - z) + 1e-8)\n",
        "\n",
        "def linear_trans(x: np.ndarray, w: np.ndarray, b: np.ndarray ) -> None:\n",
        "    return np.dot(w, x) + b"
      ],
      "metadata": {
        "id": "3pK1IV0WNc7V"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_feature: int,\n",
        "            out_feature: int,\n",
        "            h1_feature: int,\n",
        "            lr: float = 0.01,\n",
        "            epochs: int = 500\n",
        "        ) -> None:\n",
        "        self.in_feature = in_feature\n",
        "        self.h1_feature = h1_feature\n",
        "        self.out_feature = out_feature\n",
        "\n",
        "        self.lr =  lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.h1_weight = None\n",
        "        self.h1_bias = None\n",
        "        self.out_weight = None\n",
        "        self.out_bias = None\n",
        "\n",
        "\n",
        "    def forward_pass(self) -> None:\n",
        "        self.Z1 = linear_trans(x=self.x.T, w=self.h1_weight, b=self.h1_bias)\n",
        "        self.A1 = relu(z=self.Z1)\n",
        "\n",
        "        self.Z2 = linear_trans(x=self.A1, w=self.out_weight, b=self.out_bias)\n",
        "        self.A2 = sigmoid(z=self.Z2)\n",
        "\n",
        "    def backward_pass(self) -> None:\n",
        "        self.dl_da2 = derivative_bce(self.A2, self.y)\n",
        "        da2_dz2 = derivative_sigmoid(self.A2)\n",
        "        dl_dz2 = self.dl_da2 * da2_dz2\n",
        "\n",
        "        dl_da1 = np.dot(self.out_weight.T, dl_dz2)  # loss\n",
        "        da1_dz1 = derivative_relu(self.Z1)  # derivative of activation\n",
        "        dl_dz1 = dl_da1 * da1_dz1\n",
        "\n",
        "        # params update\n",
        "\n",
        "        # output layer\n",
        "        # derivative of weight and bias\n",
        "        dl_dw2 = np.dot(dl_dz2, self.A1.T)\n",
        "        dl_db2 = np.sum(dl_dz2, axis=1, keepdims=True)\n",
        "        # using gradient descent ton update w and b\n",
        "        self.out_weight -= self.lr * dl_dw2\n",
        "        self.out_bias -= self.lr * dl_db2\n",
        "\n",
        "        # hidden layer\n",
        "        # derivative of weight and bias\n",
        "        dl_dw1 = np.dot(dl_dz1, self.x)\n",
        "        dl_db1 = np.sum(dl_dz1, axis=1, keepdims=True)\n",
        "        # using gradient descent ton update w and b\n",
        "        self.h1_weight -= self.lr * dl_dw1\n",
        "        self.h1_bias -= self.lr * dl_db1\n",
        "\n",
        "    def fit(self, x: np.ndarray, y: np.ndarray) -> None:\n",
        "        # initialize random weight\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.h1_weight = np.random.randn(self.h1_feature, self.in_feature) * 0.001\n",
        "        self.h1_bias = np.zeros((self.h1_feature, 1))\n",
        "\n",
        "        self.out_weight = np.random.randn(self.out_feature, self.h1_feature) * 0.001\n",
        "        self.out_bias = np.zeros((self.out_feature, 1))\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # training\n",
        "            self.forward_pass()\n",
        "            self.backward_pass()\n",
        "\n",
        "            loss = -np.mean(self.y * np.log(self.A2 + 1e-8) + (1 - self.y) * np.log(1 - self.A2 + 1e-8))\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {loss:.6f}\")\n",
        "\n",
        "    def predict(self) -> None:..."
      ],
      "metadata": {
        "id": "OGAW73iUyVVy"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.randn(1000, 10)\n",
        "y = np.array([np.random.choice([1,0]) for x in range(1000)])"
      ],
      "metadata": {
        "id": "knsZhHLyUHXB"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_feat = 10\n",
        "out_feat = 1\n",
        "h1_feat = 15\n",
        "\n",
        "mlp = MLP(\n",
        "    in_feature=in_feat,\n",
        "    h1_feature=h1_feat,\n",
        "    out_feature=out_feat,\n",
        "    lr=0.001,\n",
        "    epochs=1000\n",
        ")\n",
        "\n",
        "mlp.fit(x=x, y=y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ7ae0XyTq8v",
        "outputId": "c2cb6c97-6066-4cce-a09a-15aa2b1bd462"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.693147\n",
            "Epoch 100: Loss = 0.691176\n",
            "Epoch 200: Loss = 0.666635\n",
            "Epoch 300: Loss = 0.645757\n",
            "Epoch 400: Loss = 0.635724\n",
            "Epoch 500: Loss = 0.630148\n",
            "Epoch 600: Loss = 0.620706\n",
            "Epoch 700: Loss = 0.622932\n",
            "Epoch 800: Loss = 0.617089\n",
            "Epoch 900: Loss = 0.615498\n"
          ]
        }
      ]
    }
  ]
}