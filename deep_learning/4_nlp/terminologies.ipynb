{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGSdZbogKZzZY3QYiSBWVE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/machine-learning-notes/blob/main/deep_learning/4_nlp/terminologies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Terminologies in NLP**"
      ],
      "metadata": {
        "id": "RgKGy96SKLsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "\n",
        "test_text: str = \"I'm Walking through a lake, but i did'nt like it!\"\n",
        "test_text = test_text.lower()"
      ],
      "metadata": {
        "id": "-IBbC7_lLTpA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep_y8q_4MjD1",
        "outputId": "77c1c073-bcde-4232-dc24-660294f41598"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.5.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
            "Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pre-trained model for nlp tasks\n",
        "nltk.download('punkt_ta')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "1iKlRW7i2FKd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4d745d-638b-458b-9932-808fe6d0926a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading punkt_ta: Package 'punkt_ta' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**: splitting text into words.(unlike split word with spaces, it split spaces and special char)"
      ],
      "metadata": {
        "id": "bep-gOObKt6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRxvtvXJImED",
        "outputId": "32972c36-ea56-4eea-bd16-f5899be5ebcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i'm\",\n",
              " 'walking',\n",
              " 'through',\n",
              " 'a',\n",
              " 'lake,',\n",
              " 'but',\n",
              " 'i',\n",
              " \"did'nt\",\n",
              " 'like',\n",
              " 'it!']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "re.split(\"\\s\", test_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with nltk\n",
        "words = nltk.word_tokenize(test_text)\n",
        "words"
      ],
      "metadata": {
        "id": "6scDhXkqRoQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b50e71-0ec2-4d2e-8463-348838b37481"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " \"'m\",\n",
              " 'walking',\n",
              " 'through',\n",
              " 'a',\n",
              " 'lake',\n",
              " ',',\n",
              " 'but',\n",
              " 'i',\n",
              " \"did'nt\",\n",
              " 'like',\n",
              " 'it',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming** : reducing word to their root form (mostly it just cutout the postfix words such as 'ing' 'ed' 's' from the word)"
      ],
      "metadata": {
        "id": "CfQbKeAWMbXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g.\n",
        "# running -> runn\n",
        "# Natural -> Natur"
      ],
      "metadata": {
        "id": "goN507V4LpVD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\" \".join([ps.stem(each) for each in words])  # stemmed text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BVnFMtufUGqH",
        "outputId": "27854fae-144a-45ad-b1cd-349751c11019"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i 'm walk through a lake , but i did'nt like it !\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part Of Speech (POS)**: identify and classify word as noun, verbe .. and so on"
      ],
      "metadata": {
        "id": "yDy4dXOVBPXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "\n",
        "word_pos = pos_tag(words)\n",
        "word_pos"
      ],
      "metadata": {
        "id": "9ptiT0uRzzbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16c87426-78d0-483f-c7f0-82d317693fb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'NN'),\n",
              " (\"'m\", 'VBP'),\n",
              " ('walking', 'VBG'),\n",
              " ('through', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('lake', 'NN'),\n",
              " (',', ','),\n",
              " ('but', 'CC'),\n",
              " ('i', 'JJ'),\n",
              " (\"did'nt\", 'VBP'),\n",
              " ('like', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**: covert word into their meaningfull form (it give more meaningfull and dictionary based word)"
      ],
      "metadata": {
        "id": "-uL6dDrRzlEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Before apply lemmatization its better perform pos_tag first, bcz the lemmatization need post tag of word to perform well.**\n",
        "- **only need to lemmatize 'NOUN', 'VERB', 'ADVERB', 'ADJECTIVE'**"
      ],
      "metadata": {
        "id": "xCBazWXDdvOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g.\n",
        "# better -> good"
      ],
      "metadata": {
        "id": "t9En08bRzyPa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "pos_result = []\n",
        "for each in word_pos:\n",
        "    pt = each[1][0]\n",
        "    match pt:\n",
        "        case \"V\":\n",
        "            pos_result.append((each[0], wordnet.VERB))\n",
        "        case \"N\":\n",
        "            pos_result.append((each[0], wordnet.NOUN))\n",
        "        case \"J\":\n",
        "            pos_result.append((each[0], wordnet.ADJ))\n",
        "        case _:\n",
        "            pos_result.append((each[0],))\n",
        "\n",
        "\n",
        "pos_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBW8aXnpWBwk",
        "outputId": "41e7efc6-4da0-4405-87d7-7395a65eeb1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'n'),\n",
              " (\"'m\", 'v'),\n",
              " ('walking', 'v'),\n",
              " ('through',),\n",
              " ('a',),\n",
              " ('lake', 'n'),\n",
              " (',',),\n",
              " ('but',),\n",
              " ('i', 'a'),\n",
              " (\"did'nt\", 'v'),\n",
              " ('like',),\n",
              " ('it',),\n",
              " ('!',)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "\" \".join([wl.lemmatize(each[0], pos=each[1] if len(each) >= 2 else \"n\") for each in pos_result])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fw85H-g1cAC2",
        "outputId": "3287408c-73b9-4e09-988a-1944ca2e91c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i 'm walk through a lake , but i did'nt like it !\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Named Entity Recognition (NER)** : add lables on word, such as names, date .. so on"
      ],
      "metadata": {
        "id": "SQhEvYYpxqG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "text = \"My self Muhammed Adnan, i'm born on December 29, 2004 at India\"\n",
        "token = nltk.word_tokenize(text)\n",
        "my_pos = pos_tag(token)\n",
        "tree = ne_chunk(my_pos)\n",
        "tree"
      ],
      "metadata": {
        "id": "zh8VgfIuZLp3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "359a21e5-582c-4468-a1a5-a7a70c5c6bff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [('My', 'PRP$'), ('self', 'NN'), Tree('PERSON', [('Muhammed', 'NNP'), ('Adnan', 'NNP')]), (',', ','), ('i', 'NN'), (\"'m\", 'VBP'), ('born', 'VBN'), ('on', 'IN'), ('December', 'NNP'), ('29', 'CD'), (',', ','), ('2004', 'CD'), ('at', 'IN'), Tree('GPE', [('India', 'NNP')])])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,680.0,168.0\" width=\"680px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"7.05882%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">My</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PRP$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.52941%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.05882%\" x=\"7.05882%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">self</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5882%\" y1=\"20px\" y2=\"48px\" /><svg width=\"20%\" x=\"14.1176%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"58.8235%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Muhammed</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.4118%\" y1=\"20px\" y2=\"48px\" /><svg width=\"41.1765%\" x=\"58.8235%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Adnan</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.4118%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"24.1176%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.52941%\" x=\"34.1176%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.8824%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.70588%\" x=\"37.6471%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">i</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.88235%\" x=\"42.3529%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">'m</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.2941%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.05882%\" x=\"48.2353%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">born</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.7647%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.70588%\" x=\"55.2941%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.6471%\" y1=\"20px\" y2=\"48px\" /><svg width=\"11.7647%\" x=\"60%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">December</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.8824%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.70588%\" x=\"71.7647%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">29</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.1176%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.52941%\" x=\"76.4706%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.2353%\" y1=\"20px\" y2=\"48px\" /><svg width=\"7.05882%\" x=\"80%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">2004</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.5294%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.70588%\" x=\"87.0588%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">at</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.4118%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.23529%\" x=\"91.7647%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">India</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95.8824%\" y1=\"20px\" y2=\"48px\" /></svg>"
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for each in tree:\n",
        "    if hasattr(each, \"label\"):\n",
        "        value = \" \".join([ word for word, cat in each.leaves()])\n",
        "        label = each.label()\n",
        "        print((value, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02A36Sev3VqN",
        "outputId": "04d1f0c8-2a41-4035-fcd5-8d6da7dbb9f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Muhammed Adnan', 'PERSON')\n",
            "('India', 'GPE')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag Of Words (BOW)**:\n",
        "- First, it identify all the unique words in the dataset\n",
        "- Then, it make count on each word how much time that word occure in the sentence\n",
        "- Then, it genreate a matrix with that values\n",
        "- Voccbulary (unique values)"
      ],
      "metadata": {
        "id": "8E1L5h2j1lM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"Hey John, could you please clarify me to our new Data Science Project!\",\n",
        "    \"Sure MCcathy, here the main thing that we need to implement.\"\n",
        "]\n",
        "\n",
        "cv = CountVectorizer(stop_words=\"english\")\n",
        "text_vector = cv.fit_transform(texts)\n",
        "\n",
        "print(f\"Voccabulary (unique values in the 'texts' dataset): \\n{cv.get_feature_names_out()}\")\n",
        "\n",
        "print(f\"array : \\n{text_vector.toarray()}\")"
      ],
      "metadata": {
        "id": "nAKoJjVN7SHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd4c783-b5a2-4c5b-e3c5-ff338f9440f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voccabulary (unique values in the 'texts' dataset): \n",
            "['clarify' 'data' 'hey' 'implement' 'john' 'main' 'mccathy' 'need' 'new'\n",
            " 'project' 'science' 'sure' 'thing']\n",
            "array : \n",
            "[[1 1 1 0 1 0 0 0 1 1 1 0 0]\n",
            " [0 0 0 1 0 1 1 1 0 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **voccabulary count: `23`**\n",
        "- **It convert all sentence to each vector (size: voccabulary size)**\n",
        "- **Count each word if the word in voccabulary, else it mark as 0**"
      ],
      "metadata": {
        "id": "L00MHt53QgdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF-IDF (Term Frequency & Inverse Document Frequency)**\n",
        "\n",
        "- **It reduce the importence of the word which is most occured in entire docuement. And give weight to words that are less occured in less**\n",
        "- **`Term Frequency`** : Measure the frequency of word in one docuement.\n",
        "- **`Inverse Docuement Frequency`** : Measure the rareness of a word in entire docuement, and it give importence if the word has high rareness and vise verse"
      ],
      "metadata": {
        "id": "XuUt0pBKReAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf = TfidfVectorizer(stop_words=\"english\")\n",
        "tf_idf_model = tf_idf.fit_transform(texts)"
      ],
      "metadata": {
        "id": "J9Fz5zjqNWTr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf.get_feature_names_out()\n",
        "tf_idf_model.toarray()"
      ],
      "metadata": {
        "id": "wtmIUoRifl8W",
        "outputId": "5d160cbb-62b4-4432-d282-a49f5f6b84ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.37796447, 0.37796447, 0.37796447, 0.        , 0.37796447,\n",
              "        0.        , 0.        , 0.        , 0.37796447, 0.37796447,\n",
              "        0.37796447, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.40824829, 0.        ,\n",
              "        0.40824829, 0.40824829, 0.40824829, 0.        , 0.        ,\n",
              "        0.        , 0.40824829, 0.40824829]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bkJfR-VU5GII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}