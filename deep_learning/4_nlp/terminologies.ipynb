{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYp6vUAjpOEwnXpod3mHQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adnaen/machine-learning-notes/blob/main/deep_learning/4_nlp/terminologies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Terminologies in NLP**"
      ],
      "metadata": {
        "id": "RgKGy96SKLsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we use spacy for nlp tasks over nltk, because spacy is more pythonic and have object oriented way**"
      ],
      "metadata": {
        "id": "mDvoQYCoMy_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "text: str = \"I'm Walking through Dr. Strange a lake, but i did'nt like it!.But i know one thing i have eaten thing\""
      ],
      "metadata": {
        "id": "-IBbC7_lLTpA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**: splitting text into words a.k.a tokens (unlike split word with spaces, it split spaces and special char)"
      ],
      "metadata": {
        "id": "bep-gOObKt6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLl8B4DgXWNY",
        "outputId": "398ec3c1-963a-43ea-c3f6-7a53df21decc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "'m\n",
            "Walking\n",
            "through\n",
            "Dr.\n",
            "Strange\n",
            "a\n",
            "lake\n",
            ",\n",
            "but\n",
            "i\n",
            "did'nt\n",
            "like\n",
            "it!.But\n",
            "i\n",
            "know\n",
            "one\n",
            "thing\n",
            "i\n",
            "have\n",
            "eaten\n",
            "thing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming** : reducing word to their root form (mostly it just cutout the suffix words such as 'ing' 'ed' 's' from the word)"
      ],
      "metadata": {
        "id": "CfQbKeAWMbXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g.\n",
        "# running -> run\n",
        "# Natural -> Natur\n",
        "# eaten -> eaten\n",
        "# ate -> ate"
      ],
      "metadata": {
        "id": "goN507V4LpVD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\" \".join([ps.stem(token) for token in [\"running\", \"natural\", \"eaten\", \"eat\", \"ate\"]])  # stemmed text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BVnFMtufUGqH",
        "outputId": "f5b48e03-ecf6-451b-a900-89882cb583e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'run natur eaten eat ate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy does not have  stemming option, bcz its Lemmetization is enough to get accurate base token**"
      ],
      "metadata": {
        "id": "yqMWhW1CGvLd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**: covert tokens into their base form, work similar to stemming but lemmatization give more meaningfull base token"
      ],
      "metadata": {
        "id": "-uL6dDrRzlEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g.\n",
        "# better -> well\n",
        "# eaten -> eat\n",
        "# ate -> eat"
      ],
      "metadata": {
        "id": "t9En08bRzyPa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "qG8l2mwHHlre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c72603b-dadc-400d-d7b7-5c05d90c0984"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in top cell, we just download the pre-trained english language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Adnan is running after ate launch, and he was good at it.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token.text}\\t | {token.lemma_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXDC85ifHX5H",
        "outputId": "e1bfebe9-218e-43d4-84ca-484bd028b7db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adnan\t | Adnan\n",
            "is\t | be\n",
            "running\t | run\n",
            "after\t | after\n",
            "ate\t | ate\n",
            "launch\t | launch\n",
            ",\t | ,\n",
            "and\t | and\n",
            "he\t | he\n",
            "was\t | be\n",
            "good\t | good\n",
            "at\t | at\n",
            "it\t | it\n",
            ".\t | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part Of Speech (POS)**: identify and classify word as noun, verbe .. and so on"
      ],
      "metadata": {
        "id": "yDy4dXOVBPXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Adnan is running after ate launch, and he was good at it.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"{token}\\t | {token.pos_} ({spacy.explain(token.pos_)}) | {token.tag_} ({spacy.explain(token.tag_)})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lw7SF_tBJokq",
        "outputId": "fff1af63-690f-4cb3-e98e-c06c241016bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adnan\t | PROPN (proper noun) | NNP (noun, proper singular)\n",
            "is\t | AUX (auxiliary) | VBZ (verb, 3rd person singular present)\n",
            "running\t | VERB (verb) | VBG (verb, gerund or present participle)\n",
            "after\t | ADP (adposition) | IN (conjunction, subordinating or preposition)\n",
            "ate\t | ADJ (adjective) | JJ (adjective (English), other noun-modifier (Chinese))\n",
            "launch\t | NOUN (noun) | NN (noun, singular or mass)\n",
            ",\t | PUNCT (punctuation) | , (punctuation mark, comma)\n",
            "and\t | CCONJ (coordinating conjunction) | CC (conjunction, coordinating)\n",
            "he\t | PRON (pronoun) | PRP (pronoun, personal)\n",
            "was\t | AUX (auxiliary) | VBD (verb, past tense)\n",
            "good\t | ADJ (adjective) | JJ (adjective (English), other noun-modifier (Chinese))\n",
            "at\t | ADP (adposition) | IN (conjunction, subordinating or preposition)\n",
            "it\t | PRON (pronoun) | PRP (pronoun, personal)\n",
            ".\t | PUNCT (punctuation) | . (punctuation mark, sentence closer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Named Entity Recognition (NER)** : categorize tokens into PERSON, MONEY, DATE ... so on"
      ],
      "metadata": {
        "id": "SQhEvYYpxqG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"As CEO of both Alphabet and Google in 2023, Sundar Pichai earned a salary of $2 million annually\")\n",
        "\n",
        "for token in doc.ents:\n",
        "    print(f\"{token.text} | {token.label_}\")\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "kl3UrjKIKKzc",
        "outputId": "d15e67d9-10bc-4c5d-f2bf-bccfb286dc83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alphabet and Google | ORG\n",
            "2023 | DATE\n",
            "Sundar Pichai | PERSON\n",
            "$2 million | MONEY\n",
            "annually | DATE\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">As CEO of both \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Alphabet and Google\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2023\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sundar Pichai\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " earned a salary of \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $2 million\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    annually\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag Of Words (BOW)**:\n",
        "- First, it identify all the unique words in the dataset\n",
        "- Then, it make count on each word how much time that word occure in the sentence\n",
        "- Then, it genreate a matrix with that values\n",
        "- Voccbulary (unique values)"
      ],
      "metadata": {
        "id": "8E1L5h2j1lM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "texts = [\n",
        "    \"Hey John, could you please clarify me to our new Data Science Project!\",\n",
        "    \"Sure MCcathy, here the main thing that we need to implement.\"\n",
        "]\n",
        "\n",
        "cv = CountVectorizer(stop_words=\"english\")\n",
        "text_vector = cv.fit_transform(texts)\n",
        "\n",
        "print(f\"Voccabulary (unique values in the 'texts' dataset): \\n{cv.get_feature_names_out()}\")\n",
        "\n",
        "print(f\"array : \\n{text_vector.toarray()}\")"
      ],
      "metadata": {
        "id": "nAKoJjVN7SHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ad86c6-0cd3-48f9-f8e1-a4bad35c29ad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voccabulary (unique values in the 'texts' dataset): \n",
            "['clarify' 'data' 'hey' 'implement' 'john' 'main' 'mccathy' 'need' 'new'\n",
            " 'project' 'science' 'sure' 'thing']\n",
            "array : \n",
            "[[1 1 1 0 1 0 0 0 1 1 1 0 0]\n",
            " [0 0 0 1 0 1 1 1 0 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **voccabulary count: `23`**\n",
        "- **It convert all sentence to each vector (size: voccabulary size)**\n",
        "- **Count each word if the word in voccabulary, else it mark as 0**"
      ],
      "metadata": {
        "id": "L00MHt53QgdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TF-IDF (Term Frequency & Inverse Document Frequency)**\n",
        "\n",
        "- **It reduce the importence of the word which is most occured in entire docuement. And give weight to words that are less occured in less**\n",
        "- **`Term Frequency`** : Measure the frequency of word in one docuement.\n",
        "- **`Inverse Docuement Frequency`** : Measure the rareness of a word in entire docuement, and it give importence if the word has high rareness and vise verse"
      ],
      "metadata": {
        "id": "XuUt0pBKReAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf_idf = TfidfVectorizer(stop_words=\"english\")\n",
        "tf_idf_model = tf_idf.fit_transform(texts)"
      ],
      "metadata": {
        "id": "J9Fz5zjqNWTr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf.get_feature_names_out()\n",
        "tf_idf_model.toarray()"
      ],
      "metadata": {
        "id": "wtmIUoRifl8W",
        "outputId": "73e44a1c-270f-48fd-d247-f3e9caa62835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.37796447, 0.37796447, 0.37796447, 0.        , 0.37796447,\n",
              "        0.        , 0.        , 0.        , 0.37796447, 0.37796447,\n",
              "        0.37796447, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.40824829, 0.        ,\n",
              "        0.40824829, 0.40824829, 0.40824829, 0.        , 0.        ,\n",
              "        0.        , 0.40824829, 0.40824829]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4ZXJ-04Xvs2"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}